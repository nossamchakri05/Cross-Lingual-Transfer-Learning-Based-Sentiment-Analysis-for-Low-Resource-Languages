{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e94b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7\n",
    "#English\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Load cleaned English sentiment dataset\n",
    "english_df = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "# Preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "english_df['review'] = english_df['review'].apply(clean_text)\n",
    "english_df = english_df.rename(columns={'review': 'text', 'sentiment': 'label'})\n",
    "english_df['label'] = english_df['label'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Check\n",
    "print(english_df.head())\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        return {'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'label': torch.tensor(label)}\n",
    "\n",
    "# Create Dataset\n",
    "dataset = SentimentDataset(english_df['text'].tolist(), english_df['label'].tolist())\n",
    "\n",
    "# Split train-test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Evaluate after each epoch\n",
    "    acc, prec, rec, f1 = evaluate(model, test_loader)\n",
    "    print(f\"\\nEvaluation after Epoch {epoch+1}:\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "print(\"Training completed ✅\")\n",
    "\n",
    "# Save the model\n",
    "save_path = './sentiment_model'\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"Model saved at {save_path} ✅\")\n",
    "\n",
    "# -------------------\n",
    "# Prediction on new sentences\n",
    "def predict_sentiment(sentences):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "    return preds.cpu().numpy()\n",
    "\n",
    "# Test Predictions\n",
    "test_sentences = [\n",
    "    \"I absolutely loved the movie! It was fantastic.\",\n",
    "    \"The film was too slow and boring.\",\n",
    "    \"Amazing acting and a gripping story!\",\n",
    "    \"I didn't enjoy the movie at all.\"\n",
    "]\n",
    "\n",
    "predictions = predict_sentiment(test_sentences)\n",
    "\n",
    "for sentence, pred in zip(test_sentences, predictions):\n",
    "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    print(f\"Sentence: {sentence}\\nPredicted Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc5305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#English testing\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = './sentiment_model'\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Preprocessing function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Load and preprocess dataset\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "df['review'] = df['review'].apply(clean_text)\n",
    "df = df.rename(columns={'review': 'text', 'sentiment': 'label'})\n",
    "df['label'] = df['label'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Select 20 random samples\n",
    "samples = df.sample(n=20, random_state=42)\n",
    "\n",
    "# Prediction function\n",
    "def predict_sentiment(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "    return preds.cpu().numpy()\n",
    "\n",
    "# Predict and print\n",
    "texts = samples['text'].tolist()\n",
    "true_labels = samples['label'].tolist()\n",
    "predictions = predict_sentiment(texts)\n",
    "\n",
    "for i in range(20):\n",
    "    predicted = \"Positive\" if predictions[i] == 1 else \"Negative\"\n",
    "    actual = \"Positive\" if true_labels[i] == 1 else \"Negative\"\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Text: {texts[i]}\")\n",
    "    print(f\"Predicted Sentiment: {predicted} | Actual Sentiment: {actual}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f6979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spanish\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset (which only has the 'test' split)\n",
    "dataset = load_dataset(\"TheFinAI/flare-es-tsa\")\n",
    "\n",
    "# Convert to Pandas DataFrame (use 'test' split)\n",
    "spanish_df = dataset['test'].to_pandas()\n",
    "\n",
    "# Preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-ZáéíóúüñÁÉÍÓÚÜÑ0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "spanish_df['text'] = spanish_df['text'].apply(clean_text)\n",
    "\n",
    "spanish_df = spanish_df.rename(columns={'answer': 'label'}) \n",
    "# Map sentiment strings to integers\n",
    "label_mapping = {'negativo': 0, 'neutral': 1, 'positivo': 2}\n",
    "spanish_df['label'] = spanish_df['label'].map(label_mapping)\n",
    "\n",
    "# Check\n",
    "print(spanish_df.head())\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        return {'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'label': torch.tensor(label)}\n",
    "\n",
    "# Split dataset into train and test (80% train, 20% test)\n",
    "train_size = int(0.8 * len(spanish_df))\n",
    "test_size = len(spanish_df) - train_size\n",
    "train_df = spanish_df[:train_size]\n",
    "test_df = spanish_df[train_size:]\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = SentimentDataset(train_df['text'].tolist(), train_df['label'].tolist())\n",
    "test_dataset = SentimentDataset(test_df['text'].tolist(), test_df['label'].tolist())\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Load multilingual BERT model for 3 classes\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Evaluation function (macro average for multiclass)\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Evaluate after each epoch\n",
    "    acc, prec, rec, f1 = evaluate(model, test_loader)\n",
    "    print(f\"\\nEvaluation after Epoch {epoch+1}:\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "print(\"Training completed ✅\")\n",
    "\n",
    "# Save the model\n",
    "save_path = './spanish_sentiment_model'\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"Model saved at {save_path} ✅\")\n",
    "\n",
    "# -------------------\n",
    "# Prediction on new Spanish sentences\n",
    "def predict_sentiment(sentences):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "    return preds.cpu().numpy()\n",
    "\n",
    "# Example Spanish sentences\n",
    "test_sentences = [\n",
    "    \"El mercado bursátil cerró con una subida inesperada.\",\n",
    "    \"La empresa anunció pérdidas trimestrales significativas.\",\n",
    "    \"Los resultados fueron neutrales, sin grandes sorpresas.\"\n",
    "]\n",
    "\n",
    "predictions = predict_sentiment(test_sentences)\n",
    "\n",
    "label_reverse_map = {0: \"Negativo\", 1: \"Neutral\", 2: \"Positivo\"}\n",
    "\n",
    "for sentence, pred in zip(test_sentences, predictions):\n",
    "    sentiment = label_reverse_map[pred]\n",
    "    print(f\"Sentence: {sentence}\\nPredicted Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c7db65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentimiento predicho: Negativo\n"
     ]
    }
   ],
   "source": [
    "#Spanish Testing \n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load saved model and tokenizer\n",
    "model_path = './spanish_sentiment_model'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Prediction function\n",
    "def predict_sentiment(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    pred = torch.argmax(probs, dim=1).item()\n",
    "    return pred\n",
    "\n",
    "# Reverse map for labels\n",
    "label_reverse_map = {0: \"Negativo\", 1: \"Neutral\", 2: \"Positivo\"}\n",
    "\n",
    "# Input sentence from user\n",
    "user_input = input(\"Introduce una oración en español para analizar el sentimiento: \")\n",
    "prediction = predict_sentiment(user_input)\n",
    "print(f\"Sentimiento predicho: {label_reverse_map[prediction]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#French\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the allocine dataset\n",
    "dataset = load_dataset(\"allocine\")\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Convert to pandas DataFrame for easier processing\n",
    "train_df = pd.DataFrame({\"text\": train_data[\"review\"], \"label\": train_data[\"label\"]})\n",
    "test_df = pd.DataFrame({\"text\": test_data[\"review\"], \"label\": test_data[\"label\"]})\n",
    "\n",
    "# Preprocessing function for French text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    # Keep French accents and special characters\n",
    "    text = re.sub(r\"[^a-zA-ZÀ-ÿ0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "# Create balanced subset (25k positive + 25k negative = 50k total)\n",
    "positive_samples = train_df[train_df['label'] == 1].sample(n=25000, random_state=42)\n",
    "negative_samples = train_df[train_df['label'] == 0].sample(n=25000, random_state=42)\n",
    "balanced_train_df = pd.concat([positive_samples, negative_samples]).sample(frac=1, random_state=42)  # Shuffle\n",
    "\n",
    "print(f\"Final training set size: {len(balanced_train_df)}\")\n",
    "print(balanced_train_df['label'].value_counts())\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Tokenizer and model (using multilingual BERT)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        return {'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'label': torch.tensor(label)}\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = SentimentDataset(balanced_train_df['text'].tolist(), balanced_train_df['label'].tolist())\n",
    "test_dataset = SentimentDataset(test_df['text'].tolist(), test_df['label'].tolist())\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Evaluate after each epoch\n",
    "    acc, prec, rec, f1 = evaluate(model, test_loader)\n",
    "    print(f\"\\nEvaluation after Epoch {epoch+1}:\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "print(\"Training completed ✅\")\n",
    "\n",
    "# -------------------\n",
    "# Prediction on first 20 test samples\n",
    "def predict_sentiment(texts):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    preds = torch.argmax(outputs.logits, dim=1)\n",
    "    return preds.cpu().numpy()\n",
    "\n",
    "# Get first 20 test samples\n",
    "test_samples = test_df.head(20)\n",
    "test_texts = test_samples['text'].tolist()\n",
    "true_labels = test_samples['label'].tolist()\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict_sentiment(test_texts)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nPredictions for first 20 test samples:\")\n",
    "print(\"-\" * 60)\n",
    "for i, (text, pred, true) in enumerate(zip(test_texts, predictions, true_labels), 1):\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"Text: {text[:150]}...\")  # Print first 150 chars\n",
    "    print(f\"Predicted: {pred} | Actual: {true}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# -------------------\n",
    "# Save the model to french_model folder\n",
    "model_dir = \"french_model\"\n",
    "os.makedirs(model_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "print(f\"\\nModel and tokenizer saved to: {model_dir}\")\n",
    "\n",
    "# Verify the saved model can be loaded\n",
    "print(\"\\nVerifying saved model can be loaded...\")\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(model_dir).to(device)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Test prediction with loaded model\n",
    "test_text = \"Ce film était incroyable, je l'ai adoré!\"\n",
    "inputs = loaded_tokenizer(test_text, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(**inputs)\n",
    "prediction = torch.argmax(outputs.logits).item()\n",
    "print(f\"\\nTest prediction with loaded model on text: '{test_text}'\")\n",
    "print(f\"Predicted sentiment: {'Positive' if prediction == 1 else 'Negative'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473506bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2822d11fe81541af98039fc5cf35bcf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:\\nlp\\.env\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shrin\\.cache\\huggingface\\hub\\datasets--allocine. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95bad9ed50f407eb02c0d3f20f984ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/60.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b92bbe6cf46443e9f3b27e1bb39fa0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/7.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e22a43d9444fa3a9436f2ddd60454d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/7.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9649214b73274f22a79c7b9fc3e6ff34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/160000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7888b47027b2444ebc563f491825c759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4749c08002740ebb59b24ad67638d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for first 20 test samples using the saved model:\n",
      "------------------------------------------------------------\n",
      "Sample 1:\n",
      "Text: magnifique épopée une belle histoire touchante avec des acteurs qui interprètent très bien leur rôles mel gibson heath ledger jason isaacs le genre de...\n",
      "Predicted: Positive | Actual: Positive\n",
      "------------------------------------------------------------\n",
      "Sample 2:\n",
      "Text: je nai pas aimé mais pourtant je lui mets 2 étoiles car lexpérience est louable rien de conventionnel ici une visite et mais jonchée didées originales...\n",
      "Predicted: Negative | Actual: Negative\n",
      "------------------------------------------------------------\n",
      "Sample 3:\n",
      "Text: un dessin animé qui brille par sa féerie et ses chansons...\n",
      "Predicted: Positive | Actual: Positive\n",
      "------------------------------------------------------------\n",
      "Sample 4:\n",
      "Text: si cest là le renouveau du cinéma français cest tout de même foutrement chiant si lobjet est très stylisé et la tension palpable le film paraît plutôt...\n",
      "Predicted: Negative | Actual: Negative\n",
      "------------------------------------------------------------\n",
      "Sample 5:\n",
      "Text: et pourtant on sen doutait second volet très mauvais sans fraîcheur et particulièrement lourdingue quel dommage...\n",
      "Predicted: Negative | Actual: Negative\n",
      "------------------------------------------------------------\n",
      "Sample 6:\n",
      "Text: vous reprendrez bien un peu dété ce film je le voyais comme un mélange de rohmer et de rozier un film de vacances jadore ça un truc beau et pur qui di...\n",
      "Predicted: Positive | Actual: Positive\n",
      "------------------------------------------------------------\n",
      "Sample 7:\n",
      "Text: bon cest pas un grand film mais on passe un bon moment avec ses ado à la recherche de lorgasme ya que les allemands pour faire des films aussi barge...\n",
      "Predicted: Positive | Actual: Positive\n",
      "------------------------------------------------------------\n",
      "Sample 8:\n",
      "Text: terrible histoire que ces êtres sans amour ces êtres lisses et frustres qui passent à côté de leur vie quelle leçon monsieur brizé vous avez tout dit ...\n",
      "Predicted: Positive | Actual: Positive\n",
      "------------------------------------------------------------\n",
      "Sample 9:\n",
      "Text: un très joli film qui ressemble à un téléfilm mais qui a le mérite dêtre émouvant et proche de ses personnages magimel est vraiment très bon et lhisto...\n",
      "Predicted: Positive | Actual: Positive\n",
      "------------------------------------------------------------\n",
      "Sample 10:\n",
      "Text: mais comment certaines personnes ont pus lui mettre 55 et donc dire indirectement que cest un chefduvre et comment atil fait pour sortir au cinéma et ...\n",
      "Predicted: Negative | Actual: Negative\n",
      "------------------------------------------------------------\n",
      "Sample 11:\n",
      "Text: le film de mon enfance il a un peu vieilli maintenant mais lours reste toujours impressionnant il est bien réel contrairement au film the revenant ce ...\n",
      "Predicted: Positive | Actual: Positive\n",
      "------------------------------------------------------------\n",
      "Sample 12:\n",
      "Text: la vie de david gale est vraiment un excellent film maitrisé et redoutablement efficace un film qui ma agréablement surprit la réalisation et la mise ...\n",
      "Predicted: Positive | Actual: Positive\n",
      "------------------------------------------------------------\n",
      "Sample 13:\n",
      "Text: on ne comprend que guère lespace et le temps dans ce grand récit un peu foutoir ce qui est dit est bien plus intéressant que ce qui nous est donné à v...\n",
      "Predicted: Negative | Actual: Negative\n",
      "------------------------------------------------------------\n",
      "Sample 14:\n",
      "Text: film rare les acteurs sont formidables ils jouent juste surtout les acteurs principaux lhistoire est touchante émouvante pour qui a un cur bien filmé ...\n",
      "Predicted: Positive | Actual: Positive\n",
      "------------------------------------------------------------\n",
      "Sample 15:\n",
      "Text: je ne men souviens plus mais jai la vague impression que cétait le vide astral et chiant en plus et vous vous en souvenezvous...\n",
      "Predicted: Negative | Actual: Negative\n",
      "------------------------------------------------------------\n",
      "Sample 16:\n",
      "Text: un tres mauvais documentaire pro cannabis qui fait passé en dérision une drogue la betise american dun état qui commercialise de la drogue comme médic...\n",
      "Predicted: Negative | Actual: Negative\n",
      "------------------------------------------------------------\n",
      "Sample 17:\n",
      "Text: eli roth est un réalisateur intéressant et redoutable defficacité bruce willis fidèle à luimême le film ne révolutionne pas le genre mais il a son cha...\n",
      "Predicted: Positive | Actual: Positive\n",
      "------------------------------------------------------------\n",
      "Sample 18:\n",
      "Text: starship troopers continue sa descente aux enfers ce troisième épisode nous livre une histoire complètement tordue et surtout des effets spéciaux qui ...\n",
      "Predicted: Negative | Actual: Negative\n",
      "------------------------------------------------------------\n",
      "Sample 19:\n",
      "Text: le récit dune fable où le sort dun éléphant lhélicoptère ici peut vous prendre toute une vie malgré quelques maladresses le film tient ses promesses d...\n",
      "Predicted: Positive | Actual: Positive\n",
      "------------------------------------------------------------\n",
      "Sample 20:\n",
      "Text: chaque petite lueur despoir engendrée par une séquence intéressante sera soufflée sans sommation par un dégueulis de haine à peine masqué et malvenu c...\n",
      "Predicted: Negative | Actual: Negative\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#French testing\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Preprocessing function (same as before)\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-ZÀ-ÿ0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Load test data from allocine\n",
    "dataset = load_dataset(\"allocine\")\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Create DataFrame and clean\n",
    "test_df = pd.DataFrame({\n",
    "    \"text\": test_data[\"review\"],\n",
    "    \"label\": test_data[\"label\"]\n",
    "})\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "# Use first 20 samples\n",
    "test_samples = test_df.head(20)\n",
    "test_texts = test_samples['text'].tolist()\n",
    "true_labels = test_samples['label'].tolist()\n",
    "\n",
    "# Load model and tokenizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertForSequenceClassification.from_pretrained(\"french_model\").to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"french_model\")\n",
    "\n",
    "# Prediction function\n",
    "def predict_sentiment(texts):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    preds = torch.argmax(outputs.logits, dim=1)\n",
    "    return preds.cpu().numpy()\n",
    "\n",
    "# Run predictions\n",
    "predictions = predict_sentiment(test_texts)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nPredictions for first 20 test samples using the saved model:\")\n",
    "print(\"-\" * 60)\n",
    "for i, (text, pred, true) in enumerate(zip(test_texts, predictions, true_labels), 1):\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"Text: {text[:150]}...\")  # Print truncated text\n",
    "    print(f\"Predicted: {'Positive' if pred == 1 else 'Negative'} | Actual: {'Positive' if true == 1 else 'Negative'}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84b109e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, f1_score, precision_score, recall_score\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1956\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1954\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1955\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1956\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m   1958\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1955\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1953\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1954\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1955\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1956\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1967\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1967\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1969\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1970\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1971\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1972\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ACT2FN\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_attn_mask_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     _prepare_4d_attention_mask_for_sdpa,\n\u001b[0;32m     34\u001b[0m     _prepare_4d_causal_attention_mask_for_sdpa,\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     37\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     38\u001b[0m     BaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     TokenClassifierOutput,\n\u001b[0;32m     46\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1955\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1953\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1954\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1955\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1956\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1967\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1967\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1969\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1970\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1971\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1972\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\generation\\utils.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcandidate_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AssistantVocabTranslatorCache\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     Cache,\n\u001b[0;32m     34\u001b[0m     DynamicCache,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     StaticCache,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\generation\\candidate_generator.py:27\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_sklearn_available\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sklearn_available():\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_curve\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DynamicCache\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m isin_mps_friendly\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\sklearn\\metrics\\__init__.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DistanceMetric\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfusion_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrixDisplay\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdet_curve\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DetCurveDisplay\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprecision_recall_curve\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PrecisionRecallDisplay\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PredictionErrorDisplay\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\sklearn\\metrics\\_plot\\det_curve.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plotting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _BinaryClassifierCurveDisplayMixin\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ranking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m det_curve\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mDetCurveDisplay\u001b[39;00m(_BinaryClassifierCurveDisplayMixin):\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"DET curve visualization.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    It is recommend to use :func:`~sklearn.metrics.DetCurveDisplay.from_estimator`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    >>> plt.show()\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data (already done in your case)\n",
    "df = pd.read_excel(\"C:/Users/ushni/OneDrive/Desktop/NLP_CODES/hindidata.ods\", engine=\"odf\", header=None)\n",
    "df.columns = ['text', 'label']\n",
    "\n",
    "# Filter Hindi text only\n",
    "def extract_hindi(text):\n",
    "    return \" \".join(re.findall(r'[\\u0900-\\u097F]+', str(text)))\n",
    "\n",
    "df['text'] = df['text'].apply(extract_hindi)\n",
    "\n",
    "# Label encoding\n",
    "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df['label'] = df['label'].map(label_map)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Show label distribution\n",
    "print(\"Label distribution:\\n\", df['label'].value_counts())\n",
    "\n",
    "# Train-test split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "\n",
    "# Convert to HuggingFace dataset format\n",
    "train_dataset = Dataset.from_dict({'text': train_texts.tolist(), 'label': train_labels.tolist()})\n",
    "val_dataset = Dataset.from_dict({'text': val_texts.tolist(), 'label': val_labels.tolist()})\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)\n",
    "\n",
    "# Evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    preds = torch.argmax(torch.tensor(p.predictions), axis=1)\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds, average='weighted'),\n",
    "        'precision': precision_score(labels, preds, average='weighted'),\n",
    "        'recall': recall_score(labels, preds, average='weighted')\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1'\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "trainer.save_model(\"hindi-sentiment-bert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d36bad",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 43\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy_score(labels, preds),\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m: f1_score(labels, preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: precision_score(labels, preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m: recall_score(labels, preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m     }\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Step 6: Define training arguments\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./bengali-finetuned-mbert\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     55\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Step 7: Fine-tune the model on Bengali data\u001b[39;00m\n\u001b[0;32m     58\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     59\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     60\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[0;32m     65\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Fine-Tune Hindi-Trained mBERT on Bengali Sentiment Data\n",
    "# -------------------------------\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import torch\n",
    "\n",
    "# Step 1: Load Bengali sentiment dataset\n",
    "bengali_dataset = load_dataset(\"Akash190104/bengali_sentiment_analysis\")['train']\n",
    "\n",
    "# Step 2: Load the same tokenizer used during Hindi training\n",
    "tokenizer = BertTokenizer.from_pretrained(\"hindi_model\")\n",
    "\n",
    "# Step 3: Tokenize Bengali text\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "bengali_dataset = bengali_dataset.map(tokenize, batched=True)\n",
    "bengali_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Optional: Split into train/eval (recommended)\n",
    "dataset_split = bengali_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "bengali_train = dataset_split['train']\n",
    "bengali_eval = dataset_split['test']\n",
    "\n",
    "# Step 4: Load the Hindi-trained model\n",
    "model = BertForSequenceClassification.from_pretrained(\"hindi_model\")\n",
    "\n",
    "# Step 5: Define evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    preds = torch.argmax(torch.tensor(p.predictions), axis=1)\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds, average='weighted'),\n",
    "        'precision': precision_score(labels, preds, average='weighted'),\n",
    "        'recall': recall_score(labels, preds, average='weighted')\n",
    "    }\n",
    "\n",
    "# Step 6: Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bengali-finetuned-mbert',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1'\n",
    ")\n",
    "\n",
    "# Step 7: Fine-tune the model on Bengali data\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=bengali_train,\n",
    "    eval_dataset=bengali_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Step 8: Save the fine-tuned Bengali model\n",
    "trainer.save_model(\"bengali-sentiment-mbert\")\n",
    "tokenizer.save_pretrained(\"bengali-sentiment-mbert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9336785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 8/8 [05:54<00:00, 44.35s/it, loss=0.0947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Evaluation: Acc=1.0000, Prec=1.0000, Rec=1.0000, F1=1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 8/8 [01:27<00:00, 10.93s/it, loss=0.0207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Evaluation: Acc=1.0000, Prec=1.0000, Rec=1.0000, F1=1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 8/8 [01:25<00:00, 10.73s/it, loss=0.0156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Evaluation: Acc=1.0000, Prec=1.0000, Rec=1.0000, F1=1.0000\n",
      "\n",
      "Afrikaans model saved to: afrikaans_finetuned_model\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Load Afrikaans dataset\n",
    "afrikaans_df = pd.read_csv(r\"T:\\nlp\\afrikaans_sentiment_75.csv\")  # Replace with the actual file path\n",
    "\n",
    "# Map sentiment labels to integers\n",
    "label_map = {\"positive\": 1, \"negative\": 0}\n",
    "afrikaans_df[\"label\"] = afrikaans_df[\"sentiment\"].map(label_map)\n",
    "\n",
    "# Clean Afrikaans text\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-zA-ZÀ-ÿ0-9\\s]\", \"\", text)  # Preserve accents and alphanumeric\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "afrikaans_df[\"text\"] = afrikaans_df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Tokenizer and Model: Load from saved French multilingual BERT\n",
    "model_dir = \"spanish_sentiment_model\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Custom Dataset\n",
    "class AfrikaansDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(self.texts[idx], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(self.labels[idx])\n",
    "        }\n",
    "\n",
    "# Split into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    afrikaans_df[\"text\"].tolist(), afrikaans_df[\"label\"].tolist(), test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = AfrikaansDataset(train_texts, train_labels)\n",
    "test_dataset = AfrikaansDataset(test_texts, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pred = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(label.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    acc, prec, rec, f1 = evaluate(model, test_loader)\n",
    "    print(f\"\\nEpoch {epoch+1} Evaluation: Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}\\n\")\n",
    "\n",
    "# Save the fine-tuned Afrikaans model\n",
    "afrikaans_model_dir = \"afrikaans_finetuned_model\"\n",
    "model.save_pretrained(afrikaans_model_dir)\n",
    "tokenizer.save_pretrained(afrikaans_model_dir)\n",
    "print(f\"Afrikaans model saved to: {afrikaans_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2cdd302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Hoy es un día muy lindo. El clima es muy agradable.\n",
      "Predicted Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the fine-tuned Afrikaans model and tokenizer\n",
    "model_dir = \"afrikaans_finetuned_model\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Function to clean and predict sentiment\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-zA-ZÀ-ÿ0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def predict_sentiment(sentence):\n",
    "    cleaned_sentence = clean_text(sentence)\n",
    "    encoding = tokenizer(cleaned_sentence, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
    "    return sentiment\n",
    "\n",
    "# Example usage\n",
    "example_sentence = \"Hoy es un día muy lindo. El clima es muy agradable.\"  # Sample Afrikaans sentence\n",
    "predicted_sentiment = predict_sentiment(example_sentence)\n",
    "print(f\"Sentence: {example_sentence}\")\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09465de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTS ---\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import re\n",
    "\n",
    "# --- Load English Sentiment Model ---\n",
    "def load_english_model():\n",
    "    model_path = './sentiment_model'\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer, device\n",
    "\n",
    "english_model, english_tokenizer, english_device = load_english_model()\n",
    "\n",
    "def clean_english_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# --- Load Spanish Model ---\n",
    "def load_spanish_model():\n",
    "    model_path = './spanish_sentiment_model'\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer, device\n",
    "\n",
    "spanish_model, spanish_tokenizer, spanish_device = load_spanish_model()\n",
    "spanish_label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "\n",
    "# --- Load French Model ---\n",
    "def load_french_model():\n",
    "    model_path = './french_model'\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer, device\n",
    "\n",
    "french_model, french_tokenizer, french_device = load_french_model()\n",
    "\n",
    "def clean_french_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-ZÀ-ÿ0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# --- Load Hindi Model ---\n",
    "def load_hindi_model():\n",
    "    model_path = './hindi_model'\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer, device\n",
    "\n",
    "hindi_model, hindi_tokenizer, hindi_device = load_hindi_model()\n",
    "hindi_label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "def extract_hindi(text):\n",
    "    return \" \".join(re.findall(r'[\\u0900-\\u097F]+', str(text)))\n",
    "\n",
    "\n",
    "# --- Load Bengali Model ---\n",
    "def load_bengali_model():\n",
    "    model_path = './bengali-sentiment-mbert'\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer, device\n",
    "\n",
    "bengali_model, bengali_tokenizer, bengali_device = load_bengali_model()\n",
    "bengali_label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "\n",
    "def extract_bengali(text):\n",
    "    return \" \".join(re.findall(r'[\\u0980-\\u09FF]+', str(text)))\n",
    "\n",
    "\n",
    "# --- Load Afrikaans Model ---\n",
    "def load_afrikaans_model():\n",
    "    model_path = './afrikaans_finetuned_model'  # using Spanish base model\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Load base model without classifier mismatch issues\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer, device\n",
    "\n",
    "\n",
    "afrikaans_model, afrikaans_tokenizer, afrikaans_device = load_afrikaans_model()\n",
    "\n",
    "def clean_afrikaans_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-ZÀ-ÿ0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# --- Load Urdu Model ---\n",
    "def load_urdu_model():\n",
    "    model_path = './urdu_finetuned_model'\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer, device\n",
    "\n",
    "urdu_model, urdu_tokenizer, urdu_device = load_urdu_model()\n",
    "\n",
    "def extract_urdu(text):\n",
    "    return \" \".join(re.findall(r'[\\u0600-\\u06FF]+', str(text)))\n",
    "\n",
    "\n",
    "# --- Load Malay Model ---\n",
    "def load_malay_model():\n",
    "    model_path = './malay_finetuned_model'\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer, device\n",
    "\n",
    "malay_model, malay_tokenizer, malay_device = load_malay_model()\n",
    "\n",
    "def clean_malay_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# --- Unified Sentiment Prediction Function ---\n",
    "def predict_sentiment_language_specific(sentence, lang):\n",
    "    lang = lang.lower()\n",
    "\n",
    "    if lang == 'en':\n",
    "        cleaned = clean_english_text(sentence)\n",
    "        inputs = english_tokenizer(cleaned, return_tensors='pt', padding=True, truncation=True, max_length=128).to(english_device)\n",
    "        with torch.no_grad():\n",
    "            outputs = english_model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "        print(f\"🧠 Sentiment Prediction (English): {sentiment}\")\n",
    "        return sentiment\n",
    "\n",
    "    elif lang == 'es':\n",
    "        inputs = spanish_tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=128).to(spanish_device)\n",
    "        with torch.no_grad():\n",
    "            outputs = spanish_model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        sentiment = spanish_label_map[pred]\n",
    "        print(f\"🧠 Sentiment Prediction (Spanish): {sentiment}\")\n",
    "        return sentiment\n",
    "\n",
    "    elif lang == 'fr':\n",
    "        cleaned = clean_french_text(sentence)\n",
    "        inputs = french_tokenizer(cleaned, return_tensors='pt', padding=True, truncation=True, max_length=128).to(french_device)\n",
    "        with torch.no_grad():\n",
    "            outputs = french_model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "        print(f\"🧠 Sentiment Prediction (French): {sentiment}\")\n",
    "        return sentiment\n",
    "\n",
    "    elif lang == 'hi' or 'or':\n",
    "        cleaned = extract_hindi(sentence)\n",
    "        inputs = hindi_tokenizer(cleaned, return_tensors='pt', padding=True, truncation=True, max_length=128).to(hindi_device)\n",
    "        with torch.no_grad():\n",
    "            outputs = hindi_model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        sentiment = hindi_label_map[pred]\n",
    "        print(f\"🧠 Sentiment Prediction (Hindi): {sentiment}\")\n",
    "        return sentiment\n",
    "\n",
    "    elif lang == 'bn':\n",
    "        cleaned = extract_bengali(sentence)\n",
    "        inputs = bengali_tokenizer(cleaned, return_tensors='pt', padding=True, truncation=True, max_length=128).to(bengali_device)\n",
    "        with torch.no_grad():\n",
    "            outputs = bengali_model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        sentiment = bengali_label_map.get(pred, \"Positive\")\n",
    "        print(f\"🧠 Sentiment Prediction (Bengali): {sentiment}\")\n",
    "        return sentiment\n",
    "\n",
    "    elif lang == 'af':\n",
    "        cleaned_sentence = clean_afrikaans_text(sentence)\n",
    "        encoding = afrikaans_tokenizer(cleaned_sentence, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "        sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
    "        return sentiment\n",
    "\n",
    "    elif lang == 'ur':\n",
    "        cleaned = extract_urdu(sentence)\n",
    "        inputs = urdu_tokenizer(cleaned, return_tensors='pt', padding=True, truncation=True, max_length=128).to(urdu_device)\n",
    "        with torch.no_grad():\n",
    "            outputs = urdu_model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "        print(f\"🧠 Sentiment Prediction (Urdu): {sentiment}\")\n",
    "        return sentiment\n",
    "\n",
    "    elif lang == 'ms':\n",
    "        cleaned = clean_malay_text(sentence)\n",
    "        inputs = malay_tokenizer(cleaned, return_tensors='pt', padding=True, truncation=True, max_length=128).to(malay_device)\n",
    "        with torch.no_grad():\n",
    "            outputs = malay_model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "        print(f\"🧠 Sentiment Prediction (Malay): {sentiment}\")\n",
    "        return sentiment\n",
    "\n",
    "    else:\n",
    "        print(f\"⚠️ Language '{lang}' not supported for sentiment analysis.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "predict_sentiment_language_specific(model_input, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feeb5acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     97\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 99\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m    102\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1675\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1667\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1669\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1673\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1675\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1682\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1683\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1687\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1689\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1144\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1144\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1156\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1157\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\pytorch_utils.py:253\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Load Urdu CSV (no header)\n",
    "urdu_df = pd.read_csv(\"urdu.csv\", header=None, names=[\"text\", \"label\"])\n",
    "\n",
    "# Map P/N to 1/0\n",
    "urdu_df[\"label\"] = urdu_df[\"label\"].map({\"P\": 1, \"N\": 0})\n",
    "\n",
    "# Clean Urdu text\n",
    "def clean_urdu(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "urdu_df[\"text\"] = urdu_df[\"text\"].apply(clean_urdu)\n",
    "\n",
    "# Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(urdu_df, test_size=0.2, random_state=42, stratify=urdu_df[\"label\"])\n",
    "\n",
    "# Load French fine-tuned model\n",
    "model_dir = \"french_model\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Freeze all BERT layers (retain previous French learning)\n",
    "for name, param in model.bert.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Dataset class\n",
    "class UrduSentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(self.texts[idx], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(self.labels[idx])\n",
    "        }\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = UrduSentimentDataset(train_df[\"text\"].tolist(), train_df[\"label\"].tolist())\n",
    "test_dataset = UrduSentimentDataset(test_df[\"text\"].tolist(), test_df[\"label\"].tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Optimizer (only train classifier head)\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\")\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    acc, prec, rec, f1 = evaluate(model, test_loader)\n",
    "    print(f\"\\nEvaluation after Epoch {epoch+1}:\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "# Save fine-tuned Urdu model\n",
    "urdu_model_dir = \"urdu_finetuned_model\"\n",
    "os.makedirs(urdu_model_dir, exist_ok=True)\n",
    "model.save_pretrained(urdu_model_dir)\n",
    "tokenizer.save_pretrained(urdu_model_dir)\n",
    "print(f\"Urdu model saved to {urdu_model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f74bdd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'negative': 0, 'positive': 1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661479ed8bbf4170a70983db58973fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 55\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy_score(labels, preds),\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m: f1_score(labels, preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: precision_score(labels, preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m: recall_score(labels, preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     52\u001b[0m     }\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Step 9: Define training arguments\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./odia-finetuned-mbert\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     67\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Step 10: Initialize Trainer\u001b[39;00m\n\u001b[0;32m     70\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     71\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     72\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[0;32m     77\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Fine-Tune Hindi-Trained mBERT on Odia Sentiment Data\n",
    "# -------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "\n",
    "# Step 1: Load Odia sentiment dataset from CSV\n",
    "df = pd.read_csv(\"odia_sentiment_100.csv\")  # Replace with your actual CSV path\n",
    "\n",
    "# Step 2: Encode 'positive' → 1, 'negative' → 0\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['sentiment'])  # Now we have a numeric 'label' column\n",
    "\n",
    "# Optional: Check the encoding\n",
    "print(\"Label mapping:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "# Step 3: Convert to HuggingFace Dataset format\n",
    "dataset = Dataset.from_pandas(df[['text', 'label']])\n",
    "\n",
    "# Step 4: Load the tokenizer (from your Hindi-trained model)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"hindi_model\")\n",
    "\n",
    "# Step 5: Tokenize Odia text\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Step 6: Split into train and eval sets\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "# Step 7: Load the pre-trained Hindi model\n",
    "model = BertForSequenceClassification.from_pretrained(\"hindi_model\")\n",
    "\n",
    "# Step 8: Define evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    preds = torch.argmax(torch.tensor(p.predictions), axis=1)\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds, average='weighted'),\n",
    "        'precision': precision_score(labels, preds, average='weighted'),\n",
    "        'recall': recall_score(labels, preds, average='weighted')\n",
    "    }\n",
    "\n",
    "# Step 9: Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./odia-finetuned-mbert',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1'\n",
    ")\n",
    "\n",
    "# Step 10: Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Step 11: Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 12: Save final model and tokenizer\n",
    "trainer.save_model(\"odia-sentiment-mbert\")\n",
    "tokenizer.save_pretrained(\"odia-sentiment-mbert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7424fe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 10/10 [05:02<00:00, 30.23s/it, loss=0.0633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Evaluation: Acc=1.0000, Prec=1.0000, Rec=1.0000, F1=1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 10/10 [01:51<00:00, 11.14s/it, loss=0.00185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Evaluation: Acc=1.0000, Prec=1.0000, Rec=1.0000, F1=1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 10/10 [01:51<00:00, 11.12s/it, loss=0.00149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Evaluation: Acc=1.0000, Prec=1.0000, Rec=1.0000, F1=1.0000\n",
      "\n",
      "Urdu model saved to: urdu_finetuned_model\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load Urdu dataset\n",
    "urdu_df = pd.read_csv(\"urdu_sentiment_100.csv\", encoding=\"utf-8\")  # Use utf-8 or try encoding=\"ISO-8859-1\" if needed\n",
    "\n",
    "# Map sentiment labels to integers\n",
    "label_map = {\"positive\": 1, \"negative\": 0}\n",
    "urdu_df[\"label\"] = urdu_df[\"sentiment\"].map(label_map)\n",
    "\n",
    "# Clean Urdu text\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^\\w\\s\\u0600-\\u06FF]\", \"\", text)  # Keep Urdu and digits\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "urdu_df[\"text\"] = urdu_df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_dir = \"french_model\"  # French mBERT checkpoint path\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Custom Dataset\n",
    "class UrduDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(self.texts[idx], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(self.labels[idx])\n",
    "        }\n",
    "\n",
    "# Train-test split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    urdu_df[\"text\"].tolist(), urdu_df[\"label\"].tolist(), test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = UrduDataset(train_texts, train_labels)\n",
    "test_dataset = UrduDataset(test_texts, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pred = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(label.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    acc, prec, rec, f1 = evaluate(model, test_loader)\n",
    "    print(f\"\\nEpoch {epoch+1} Evaluation: Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}\\n\")\n",
    "\n",
    "# Save fine-tuned Urdu model\n",
    "urdu_model_dir = \"urdu_finetuned_model\"\n",
    "model.save_pretrained(urdu_model_dir)\n",
    "tokenizer.save_pretrained(urdu_model_dir)\n",
    "print(f\"Urdu model saved to: {urdu_model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "631fe3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 8/8 [02:42<00:00, 20.37s/it, loss=0.00789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Evaluation: Acc=1.0000, Prec=1.0000, Rec=1.0000, F1=1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 8/8 [01:32<00:00, 11.50s/it, loss=0.00249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Evaluation: Acc=1.0000, Prec=1.0000, Rec=1.0000, F1=1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 8/8 [01:20<00:00, 10.08s/it, loss=0.00188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Evaluation: Acc=1.0000, Prec=1.0000, Rec=1.0000, F1=1.0000\n",
      "\n",
      "Malay model saved to: malay_finetuned_model\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load Malay dataset\n",
    "malay_df = pd.read_csv(\"malay_sentiment_75.csv\", encoding=\"utf-8\")  # Change filename if needed\n",
    "\n",
    "# Map sentiment labels\n",
    "label_map = {\"positive\": 1, \"negative\": 0}\n",
    "malay_df[\"label\"] = malay_df[\"sentiment\"].map(label_map)\n",
    "\n",
    "# Text preprocessing\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "malay_df[\"text\"] = malay_df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Load tokenizer and model (French multilingual BERT)\n",
    "model_dir = \"french_model\"  # Update if model checkpoint is in another folder\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Custom dataset\n",
    "class MalayDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(self.texts[idx], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(self.labels[idx])\n",
    "        }\n",
    "\n",
    "# Train-test split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    malay_df[\"text\"].tolist(), malay_df[\"label\"].tolist(), test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = MalayDataset(train_texts, train_labels)\n",
    "test_dataset = MalayDataset(test_texts, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pred = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(label.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    acc, prec, rec, f1 = evaluate(model, test_loader)\n",
    "    print(f\"\\nEpoch {epoch+1} Evaluation: Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}\\n\")\n",
    "\n",
    "# Save fine-tuned model\n",
    "malay_model_dir = \"malay_finetuned_model\"\n",
    "model.save_pretrained(malay_model_dir)\n",
    "tokenizer.save_pretrained(malay_model_dir)\n",
    "print(f\"Malay model saved to: {malay_model_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
