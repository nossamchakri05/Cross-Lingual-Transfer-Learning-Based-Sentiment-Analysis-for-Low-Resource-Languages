{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: or, Confidence: 0.99\n"
     ]
    }
   ],
   "source": [
    "#Step 1 and 2\n",
    "import fasttext\n",
    "\n",
    "# Load the fastText pre-trained language detection model\n",
    "model = fasttext.load_model(r\"T:\\nlp\\lid.176.bin\")\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detects the language of the given text using fastText.\n",
    "    :param text: Input sentence\n",
    "    :return: Detected language code and confidence score\n",
    "    \"\"\"\n",
    "    prediction = model.predict(text)\n",
    "    language = prediction[0][0].replace(\"__label__\", \"\")  # Extract language code\n",
    "    confidence = prediction[1][0]  # Extract confidence score\n",
    "    return language, confidence\n",
    "\n",
    "# Example usage\n",
    "sentence = input(\"Enter the sentence\")\n",
    "lang, confidence = detect_language(sentence)\n",
    "print(f\"Detected Language: {lang}, Confidence: {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Load the graph from GML file\n",
    "G = nx.read_gml(r\"T:\\nlp\\language_similarity_graph.gml\")  # Replace with your actual filename\n",
    "\n",
    "# Map language codes to correct graph node names\n",
    "language_map = {\n",
    "    'bn': 'bengali',  # Bengali\n",
    "    'or': 'odia',  # Odia\n",
    "    'af': 'afrikaans',  # Afrikaans (replaced Konkani)\n",
    "    'ms': 'malay',  # Malay (replaced Georgian)\n",
    "    'ur': 'urdu',  # Urdu\n",
    "    'en': 'english',  # English\n",
    "    'hi': 'hindi',  # Hindi\n",
    "    'fr': 'french',  # French\n",
    "    'es': 'spanish',  # Spanish\n",
    "    'de': 'german',  # German\n",
    "    'ar': 'arabic'  # Arabic\n",
    "}\n",
    "\n",
    "high_resource = ['en', 'hi', 'fr', 'es', 'de', 'ar']\n",
    "\n",
    "def find_high_resource_match(language_code):\n",
    "    if language_code not in language_map:\n",
    "        print(f\"Error: {language_code} not found in language map!\")\n",
    "        return None\n",
    "\n",
    "    sims = []\n",
    "    low_node = language_map[language_code]\n",
    "\n",
    "    for high_lang in high_resource:\n",
    "        high_node = language_map[high_lang]\n",
    "\n",
    "        if G.has_node(low_node) and G.has_node(high_node):\n",
    "            try:\n",
    "                sim = G[low_node][high_node]['weight']\n",
    "                sims.append((high_lang, sim))  # Save original HRL code\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    sims = sorted(sims, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if sims:\n",
    "        return sims[0][0]  # Return only the first high-resource language code\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "low_resource_input = lang  # Change this to any low-resource language code\n",
    "high_resource_output = find_high_resource_match(low_resource_input)\n",
    "\n",
    "if high_resource_output:\n",
    "    print(high_resource_output.lower())  # Print only the language code\n",
    "else:\n",
    "    print(\"No match found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf983f85766425681514ecd94cacc17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.2 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\shrin\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-cometkiwi-da\\snapshots\\1ad785194e391eebc6c53e2d0776cada8f83179a\\checkpoints\\model.ckpt`\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x0000017552861030>>\n",
      "Traceback (most recent call last):\n",
      "  File \"t:\\nlp\\.env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Load COMET\u001b[39;00m\n\u001b[0;32m     26\u001b[0m comet_model_path \u001b[38;5;241m=\u001b[39m download_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnbabel/wmt22-cometkiwi-da\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m comet_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomet_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Language Code Mappings\u001b[39;00m\n\u001b[0;32m     30\u001b[0m google_to_nllb \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mben_Beng\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mor\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mory_Orya\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mar\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marb_Arab\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     42\u001b[0m }\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\comet\\models\\__init__.py:120\u001b[0m, in \u001b[0;36mload_from_checkpoint\u001b[1;34m(checkpoint_path, reload_hparams, strict, local_files_only)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_softmax:\n\u001b[0;32m    118\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer_transformation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 120\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_class\u001b[38;5;241m.\u001b[39mload_from_checkpoint(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m     )\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\pytorch_lightning\\core\\module.py:1581\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1579\u001b[0m \n\u001b[0;32m   1580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1581\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m _load_from_checkpoint(\n\u001b[0;32m   1582\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   1583\u001b[0m         checkpoint_path,\n\u001b[0;32m   1584\u001b[0m         map_location,\n\u001b[0;32m   1585\u001b[0m         hparams_file,\n\u001b[0;32m   1586\u001b[0m         strict,\n\u001b[0;32m   1587\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1588\u001b[0m     )\n\u001b[0;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:91\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m---> 91\u001b[0m     model \u001b[38;5;241m=\u001b[39m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, strict\u001b[38;5;241m=\u001b[39mstrict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:165\u001b[0m, in \u001b[0;36m_load_state\u001b[1;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cls_spec\u001b[38;5;241m.\u001b[39mvarkw:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# filter kwargs according to class init unless it allows any argument via kwargs\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     _cls_kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _cls_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m cls_init_args_name}\n\u001b[1;32m--> 165\u001b[0m obj \u001b[38;5;241m=\u001b[39m instantiator(\u001b[38;5;28mcls\u001b[39m, _cls_kwargs) \u001b[38;5;28;01mif\u001b[39;00m instantiator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_cls_kwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, pl\u001b[38;5;241m.\u001b[39mLightningDataModule):\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint:\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\comet\\models\\multitask\\unified_metric.py:126\u001b[0m, in \u001b[0;36mUnifiedMetric.__init__\u001b[1;34m(self, nr_frozen_epochs, keep_embeddings_frozen, optimizer, warmup_steps, encoder_learning_rate, learning_rate, layerwise_decay, encoder_model, pretrained_model, sent_layer, layer_transformation, layer_norm, word_layer, loss, dropout, batch_size, train_data, validation_data, hidden_sizes, activations, final_activation, input_segments, word_level_training, loss_lambda, error_labels, cross_entropy_weights, load_pretrained_weights, local_files_only)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     97\u001b[0m     nr_frozen_epochs: Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m     local_files_only: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnr_frozen_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnr_frozen_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_embeddings_frozen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_embeddings_frozen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_learning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_learning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayerwise_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayerwise_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msent_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_transformation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_transformation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_identifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munified_metric\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_pretrained_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_pretrained_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters()\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator \u001b[38;5;241m=\u001b[39m FeedForward(\n\u001b[0;32m    150\u001b[0m         in_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39moutput_units,\n\u001b[0;32m    151\u001b[0m         hidden_sizes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mhidden_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m         final_activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mfinal_activation,\n\u001b[0;32m    155\u001b[0m     )\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\comet\\models\\base.py:121\u001b[0m, in \u001b[0;36mCometModel.__init__\u001b[1;34m(self, nr_frozen_epochs, keep_embeddings_frozen, optimizer, warmup_steps, encoder_learning_rate, learning_rate, layerwise_decay, encoder_model, pretrained_model, pool, layer, layer_transformation, layer_norm, loss, dropout, batch_size, train_data, validation_data, class_identifier, load_pretrained_weights, local_files_only)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mstr2encoder\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_model\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_pretrained_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_nr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mlayer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmix\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\comet\\encoders\\xlmr.py:90\u001b[0m, in \u001b[0;36mXLMREncoder.from_pretrained\u001b[1;34m(cls, pretrained_model, load_pretrained_weights, local_files_only)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     local_files_only: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     78\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Encoder:\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Function that loads a pretrained encoder from Hugging Face.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m        Encoder: XLMREncoder object.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mXLMREncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_pretrained_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\comet\\encoders\\xlmr.py:54\u001b[0m, in \u001b[0;36mXLMREncoder.__init__\u001b[1;34m(self, pretrained_model, load_pretrained_weights, local_files_only)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m XLMRobertaModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     51\u001b[0m         pretrained_model, add_pooling_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     )\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mXLMRobertaModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXLMRobertaConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_pooling_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39moutput_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:813\u001b[0m, in \u001b[0;36mXLMRobertaModel.__init__\u001b[1;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 813\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[43mXLMRobertaEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m XLMRobertaEncoder(config)\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;241m=\u001b[39m XLMRobertaPooler(config) \u001b[38;5;28;01mif\u001b[39;00m add_pooling_layer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:71\u001b[0m, in \u001b[0;36mXLMRobertaEmbeddings.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mmax_position_embeddings, config\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mtype_vocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size)\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:170\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[0;32m    167\u001b[0m         torch\u001b[38;5;241m.\u001b[39mempty((num_embeddings, embedding_dim), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs),\n\u001b[0;32m    168\u001b[0m         requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _freeze,\n\u001b[0;32m    169\u001b[0m     )\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_weight\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m [\n\u001b[0;32m    173\u001b[0m         num_embeddings,\n\u001b[0;32m    174\u001b[0m         embedding_dim,\n\u001b[0;32m    175\u001b[0m     ], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of weight does not match num_embeddings and embedding_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:182\u001b[0m, in \u001b[0;36mEmbedding.reset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     init\u001b[38;5;241m.\u001b[39mnormal_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill_padding_idx_with_zero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mt:\\nlp\\.env\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:187\u001b[0m, in \u001b[0;36mEmbedding._fill_padding_idx_with_zero\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 187\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from deep_translator import GoogleTranslator\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load NLLB\n",
    "nllb_name = \"facebook/nllb-200-distilled-600M\"\n",
    "nllb_tokenizer = AutoTokenizer.from_pretrained(nllb_name, use_fast=False)\n",
    "nllb_model = AutoModelForSeq2SeqLM.from_pretrained(nllb_name).to(device)\n",
    "lang_code_to_id = nllb_tokenizer.convert_tokens_to_ids\n",
    "\n",
    "# Load M2M100\n",
    "m2m_model_name = \"facebook/m2m100_418M\"\n",
    "m2m_tokenizer = AutoTokenizer.from_pretrained(m2m_model_name)\n",
    "m2m_model = AutoModelForSeq2SeqLM.from_pretrained(m2m_model_name).to(device)\n",
    "\n",
    "# Load MBart50\n",
    "mbart_model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "mbart_tokenizer = AutoTokenizer.from_pretrained(mbart_model_name)\n",
    "mbart_model = AutoModelForSeq2SeqLM.from_pretrained(mbart_model_name).to(device)\n",
    "\n",
    "# Load COMET\n",
    "comet_model_path = download_model(\"Unbabel/wmt22-cometkiwi-da\")\n",
    "comet_model = load_from_checkpoint(comet_model_path)\n",
    "\n",
    "# Language Code Mappings\n",
    "google_to_nllb = {\n",
    "    'bn': 'ben_Beng',\n",
    "    'or': 'ory_Orya',\n",
    "    'af': 'afr_Latn',\n",
    "    'ms': 'zsm_Latn',\n",
    "    'ur': 'urd_Arab',\n",
    "    'en': 'eng_Latn',\n",
    "    'hi': 'hin_Deva',\n",
    "    'fr': 'fra_Latn',\n",
    "    'es': 'spa_Latn',\n",
    "    'de': 'deu_Latn',\n",
    "    'ar': 'arb_Arab'\n",
    "}\n",
    "\n",
    "google_to_mbart50 = {\n",
    "    'bn': 'bn_IN',\n",
    "    'or': 'or_IN',\n",
    "    'af': 'af_ZA',\n",
    "    'ms': 'ms_MY',\n",
    "    'ur': 'ur_PK',\n",
    "    'en': 'en_XX',\n",
    "    'hi': 'hi_IN',\n",
    "    'fr': 'fr_XX',\n",
    "    'es': 'es_XX',\n",
    "    'de': 'de_DE',\n",
    "    'ar': 'ar_AR'\n",
    "}\n",
    "\n",
    "# --- Translators ---\n",
    "def translate_nllb(text, src_lang_code, tgt_lang_code):\n",
    "    try:\n",
    "        src = google_to_nllb[src_lang_code]\n",
    "        tgt = google_to_nllb[tgt_lang_code]\n",
    "        nllb_tokenizer.src_lang = src\n",
    "        forced_bos_id = lang_code_to_id(tgt)\n",
    "        encoded = nllb_tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        output = nllb_model.generate(**encoded, forced_bos_token_id=forced_bos_id, max_length=128)\n",
    "        return nllb_tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"NLLB failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def translate_google(text, src_lang_code, tgt_lang_code):\n",
    "    try:\n",
    "        return GoogleTranslator(source=src_lang_code, target=tgt_lang_code).translate(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Google failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def translate_m2m100(text, src_lang_code, tgt_lang_code):\n",
    "    try:\n",
    "        m2m_tokenizer.src_lang = src_lang_code\n",
    "        encoded = m2m_tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        generated_tokens = m2m_model.generate(**encoded, forced_bos_token_id=m2m_tokenizer.lang_code_to_id[tgt_lang_code])\n",
    "        return m2m_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"M2M100 failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def translate_mbart50(text, src_lang_code, tgt_lang_code):\n",
    "    try:\n",
    "        src = google_to_mbart50[src_lang_code]\n",
    "        tgt = google_to_mbart50[tgt_lang_code]\n",
    "        mbart_tokenizer.src_lang = src\n",
    "        encoded = mbart_tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        output = mbart_model.generate(**encoded, forced_bos_token_id=mbart_tokenizer.lang_code_to_id[tgt])\n",
    "        return mbart_tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"MBart50 failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# --- COMET Evaluation ---\n",
    "def evaluate_comet(src, mt):\n",
    "    data = [{\"src\": src, \"mt\": mt}]\n",
    "    score = comet_model.predict(data, batch_size=1, gpus=1 if torch.cuda.is_available() else 0)\n",
    "    return score[0]\n",
    "\n",
    "# --- Ensemble Translation ---\n",
    "def ensemble_translate(text, src_google_code, tgt_google_code):\n",
    "    outputs = {\n",
    "        \"nllb\": translate_nllb(text, src_google_code, tgt_google_code),\n",
    "        \"google\": translate_google(text, src_google_code, tgt_google_code),\n",
    "        \"m2m100\": translate_m2m100(text, src_google_code, tgt_google_code),\n",
    "        \"mbart50\": translate_mbart50(text, src_google_code, tgt_google_code)\n",
    "    }\n",
    "\n",
    "    print(\"\\nüîπ Translations:\")\n",
    "    for name, out in outputs.items():\n",
    "        print(f\"   {name.title():10}: {out}\")\n",
    "\n",
    "    scores = {}\n",
    "    for name, mt in outputs.items():\n",
    "        if mt.strip():\n",
    "            scores[name] = evaluate_comet(text, mt)\n",
    "        else:\n",
    "            scores[name] = float('-inf')\n",
    "\n",
    "    best_model = max(scores, key=scores.get)\n",
    "    best_translation = outputs[best_model]\n",
    "\n",
    "    print(f\"\\n‚úÖ Selected: {best_model.upper()} | COMET-QE scores: {scores}\")\n",
    "    return best_translation\n",
    "\n",
    "# --- Main Function for Translation ---\n",
    "def translate_best(sentence, low_resource_input, high_resource_output):\n",
    "    src_lang = low_resource_input.lower()\n",
    "    tgt_lang = high_resource_output.lower()\n",
    "    print(f\"\\nüìù Input Text: {sentence}\")\n",
    "    best_translation = ensemble_translate(sentence, src_lang, tgt_lang)\n",
    "    print(f\"üéØ Best Translation: {best_translation}\")\n",
    "    return best_translation\n",
    "\n",
    "# Example usage (can be replaced with user input or script call)\n",
    "# translate_best(\"‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶ó‡¶æ‡¶® ‡¶ó‡¶æ‡¶á\", \"bn\", \"en\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
